{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-03T21:16:00.429289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\r\n",
      "  Using cached spacy-3.7.2-cp39-cp39-macosx_10_9_x86_64.whl.metadata (25 kB)\r\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\r\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\r\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\r\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\r\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\r\n",
      "  Using cached murmurhash-1.0.10-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.0 kB)\r\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\r\n",
      "  Using cached cymem-2.0.8-cp39-cp39-macosx_10_9_x86_64.whl.metadata (8.4 kB)\r\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\r\n",
      "  Using cached preshed-3.0.9-cp39-cp39-macosx_10_9_x86_64.whl.metadata (2.2 kB)\r\n",
      "Collecting thinc<8.3.0,>=8.1.8 (from spacy)\r\n",
      "  Downloading thinc-8.2.1-cp39-cp39-macosx_10_9_x86_64.whl.metadata (15 kB)\r\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\r\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\r\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\r\n",
      "  Downloading srsly-2.4.8-cp39-cp39-macosx_10_9_x86_64.whl.metadata (20 kB)\r\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\r\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\r\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\r\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\r\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\r\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.9/45.9 kB\u001B[0m \u001B[31m21.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting smart-open<7.0.0,>=5.2.1 (from spacy)\r\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\r\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\r\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.6/57.6 kB\u001B[0m \u001B[31m10.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from spacy) (2.31.0)\r\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\r\n",
      "  Downloading pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m65.2/65.2 kB\u001B[0m \u001B[31m36.8 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from spacy) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from spacy) (65.5.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from spacy) (23.2)\r\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\r\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.6/181.6 kB\u001B[0m \u001B[31m31.7 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.19.0 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from spacy) (1.26.2)\r\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\r\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting pydantic-core==2.14.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\r\n",
      "  Downloading pydantic_core-2.14.5-cp39-cp39-macosx_10_7_x86_64.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\r\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\r\n",
      "  Downloading blis-0.7.11-cp39-cp39-macosx_10_9_x86_64.whl.metadata (7.4 kB)\r\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\r\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy)\r\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\r\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/xiaoyan/Desktop/spacy/venv/lib/python3.9/site-packages (from jinja2->spacy) (2.1.3)\r\n",
      "Downloading spacy-3.7.2-cp39-cp39-macosx_10_9_x86_64.whl (6.9 MB)\r\n",
      "\u001B[2K   \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.8/6.9 MB\u001B[0m \u001B[31m25.0 kB/s\u001B[0m eta \u001B[36m0:03:25\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download fr_core_news_sm\n",
    "!pip install pandas\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install pipreqs\n",
    "!pipreqs ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:30:01.617841Z",
     "start_time": "2023-12-03T21:30:00.026717Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.training.example import Example\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:32:54.128748Z",
     "start_time": "2023-12-03T21:32:54.121452Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load spaCy model and add NER labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:28:34.988112Z",
     "start_time": "2023-12-03T21:28:29.269969Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_sm')\n",
    "ner = nlp.get_pipe('ner')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:33:04.630220Z",
     "start_time": "2023-12-03T21:32:56.463286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.add_label(\"departure\")\n",
    "ner.add_label(\"arrival\")\n",
    "ner.add_label(\"transit\")\n",
    "ner.add_label(\"departure_time\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:33:06.015620Z",
     "start_time": "2023-12-03T21:33:05.975758Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and prepare data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "                     transit departure_time  \\\n0                      Melun       mercredi   \n1                      Mézin         samedi   \n2                  Aimargues           None   \n3                       None       dimanche   \n4  Feuquières-Fressenneville           None   \n\n                                            sentence          departure  \\\n0  Je réserve un billet de St-Thégonnec à St-Andr...       St-Thégonnec   \n1  Je traverse Mézin en me rendant de Courtenay à...          Courtenay   \n2  Je cherche un itinéraire de Rougebarre à Clell...         Rougebarre   \n3   Je me rends dimanche de Mézy à Lille-St-Sauveur.               Mézy   \n4  Tournon-St-Martin est mon point de départ, je ...  Tournon-St-Martin   \n\n                    arrival  \n0        St-André-de-Cubzac  \n1  Ste-Eulalie-Carbon-Blanc  \n2              Clelles-Mens  \n3          Lille-St-Sauveur  \n4                 Turckheim  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>transit</th>\n      <th>departure_time</th>\n      <th>sentence</th>\n      <th>departure</th>\n      <th>arrival</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Melun</td>\n      <td>mercredi</td>\n      <td>Je réserve un billet de St-Thégonnec à St-Andr...</td>\n      <td>St-Thégonnec</td>\n      <td>St-André-de-Cubzac</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Mézin</td>\n      <td>samedi</td>\n      <td>Je traverse Mézin en me rendant de Courtenay à...</td>\n      <td>Courtenay</td>\n      <td>Ste-Eulalie-Carbon-Blanc</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Aimargues</td>\n      <td>None</td>\n      <td>Je cherche un itinéraire de Rougebarre à Clell...</td>\n      <td>Rougebarre</td>\n      <td>Clelles-Mens</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>None</td>\n      <td>dimanche</td>\n      <td>Je me rends dimanche de Mézy à Lille-St-Sauveur.</td>\n      <td>Mézy</td>\n      <td>Lille-St-Sauveur</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Feuquières-Fressenneville</td>\n      <td>None</td>\n      <td>Tournon-St-Martin est mon point de départ, je ...</td>\n      <td>Tournon-St-Martin</td>\n      <td>Turckheim</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"./data/transport_french.json\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "df_data, df_data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:33:28.325846Z",
     "start_time": "2023-12-03T21:33:28.203367Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "('Je me rends de Pierre-Buffière à La Joux.',\n {'entities': [(15, 30, 'departure'), (33, 40, 'arrival')]})"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for index, row in df_data.iterrows():\n",
    "    sentence = row[\"sentence\"]\n",
    "    departure = row[\"departure\"]\n",
    "    arrival = row[\"arrival\"]\n",
    "    transit = row[\"transit\"]\n",
    "    departure_time = row[\"departure_time\"]\n",
    "\n",
    "    annotations = {\"entities\": []}\n",
    "\n",
    "    # Define annotations for named entities\n",
    "    if departure:\n",
    "        start_pos = sentence.find(departure)\n",
    "        end_pos = start_pos + len(departure)\n",
    "        annotations[\"entities\"].append((start_pos, end_pos, \"departure\"))\n",
    "\n",
    "    if arrival:\n",
    "        start_pos = sentence.find(arrival)\n",
    "        end_pos = start_pos + len(arrival)\n",
    "        # Check for overlap with existing entities\n",
    "        if not any(start <= start_pos < end or start < end_pos <= end for start, end, _ in annotations[\"entities\"]):\n",
    "            annotations[\"entities\"].append((start_pos, end_pos, \"arrival\"))\n",
    "\n",
    "    if transit:\n",
    "        start_pos = sentence.find(transit)\n",
    "        end_pos = start_pos + len(transit)\n",
    "        if not any(start <= start_pos < end or start < end_pos <= end for start, end, _ in annotations[\"entities\"]):\n",
    "            annotations[\"entities\"].append((start_pos, end_pos, \"transit\"))\n",
    "\n",
    "    if departure_time:\n",
    "        start_pos = sentence.find(departure_time)\n",
    "        end_pos = start_pos + len(departure_time)\n",
    "        annotations[\"entities\"].append((start_pos, end_pos, \"departure_time\"))\n",
    "\n",
    "    train_data.append((sentence, annotations))\n",
    "\n",
    "train_data[0]\n",
    "\n",
    "# for index, row in df_data.iterrows():\n",
    "#     sentence = row[\"sentence\"]\n",
    "#     departure = row[\"departure\"]\n",
    "#     arrival = row[\"arrival\"]\n",
    "#     transit = row[\"transit\"]\n",
    "#     annotations = {\n",
    "#         \"entities\": [\n",
    "#             (sentence.find(departure), sentence.find(departure) + len(departure), \"departure\"),\n",
    "#             (sentence.find(arrival), sentence.find(arrival) + len(arrival), \"arrival\"),\n",
    "#         ]\n",
    "#     }\n",
    "#\n",
    "#     if transit:\n",
    "#         annotations[\"entities\"].append(\n",
    "#             (sentence.find(transit), sentence.find(transit) + len(transit), \"transit\")\n",
    "#         )\n",
    "#\n",
    "#     train_data.append((sentence, annotations))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:33:35.824338Z",
     "start_time": "2023-12-03T21:33:34.857743Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "for text, annotations in train_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    nlp.update([example], drop=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:39:16.239329Z",
     "start_time": "2023-12-03T21:33:51.347547Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "model_path = \"./ner_transport_model\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    nlp.to_disk(model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:42:07.720955Z",
     "start_time": "2023-12-03T21:42:07.717483Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "test_examples = []\n",
    "for index, row in df_data_test.iterrows():\n",
    "    text = row[\"sentence\"]\n",
    "    entities = [(start, end, label) for start, end, label in row[\"annotations\"][\"entities\"]]\n",
    "    example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
    "    test_examples.append(example)\n",
    "\n",
    "evaluation_results = nlp.evaluate(test_examples)\n",
    "\n",
    "accuracy = evaluation_results[\"ents_per_type\"][\"accuracy\"]\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# correct_predictions = 0\n",
    "# total_predictions = 0\n",
    "#\n",
    "# for index, row in df_data_test.iterrows():\n",
    "#     my_doc = my_nlp(row[\"sentence\"])\n",
    "#     isPredictionTrue = True\n",
    "#     for ent in my_doc.ents:\n",
    "#         if ent.label_ == \"departure\" and ent.text != row[\"departure\"]:\n",
    "#             isPredictionTrue = False\n",
    "#         elif ent.label_ == \"arrival\" and ent.text != row[\"arrival\"]:\n",
    "#             isPredictionTrue = False\n",
    "#         elif ent.label_ == \"transit\" and ent.text != row[\"transit\"]:\n",
    "#             isPredictionTrue = False\n",
    "#     if isPredictionTrue:\n",
    "#         correct_predictions += 1\n",
    "#\n",
    "#     total_predictions += 1\n",
    "#\n",
    "# accuracy = correct_predictions / total_predictions\n",
    "# print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T21:59:33.175599Z",
     "start_time": "2023-12-03T21:59:13.344342Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity : Lyon, Label : transit\n",
      "Entity : Paris, Label : departure\n",
      "Entity : Marseille, Label : arrival\n",
      "Entity : 27 novembre à 9h, Label : departure_time\n"
     ]
    }
   ],
   "source": [
    "my_nlp = spacy.load(\"./ner_transport_model\")\n",
    "\n",
    "text = \"en passant par Lyon, je pars de Paris le 27 novembre à 9h pour Marseille\"\n",
    "\n",
    "# Process sample text with the trained model\n",
    "doc = my_nlp(text)\n",
    "\n",
    "# Display named entities in the processed text\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity : {ent.text}, Label : {ent.label_}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-03T22:00:39.516905Z",
     "start_time": "2023-12-03T22:00:37.640081Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
